第一篇 经典机器学习模型 第1章 引言：从线性回归说起 2 1.1 什么是机器学习 3 1.1.1 传统算法与机器学习算法 4 1.1.2 线性回归 9 1.2 过拟合与正则化 10 1.2.1 样本量与过拟合 10 1.2.2 正则化方法 12 1.3 岭回归和lasso回归 14 1.3.1 岭回归 14 1.3.2 lasso回归 17 1.3.3 l1正则化和l2正则化 17 1.4 本章小结与代码实现 21 1.5 本章话题：机器学习的 
第一篇 经典机器学习模型 
第1章 引言：从线性回归说起 2
1.1 什么是机器学习 3
1.1.1 传统算法与机器学习算法 4
1.1.2 线性回归 9
1.2 过拟合与正则化 10
1.2.1 样本量与过拟合 10
1.2.2 正则化方法 12
1.3 岭回归和lasso回归 14
1.3.1 岭回归 14
1.3.2 lasso回归 17
1.3.3 l1正则化和l2正则化 17
1.4 本章小结与代码实现 21
1.5 本章话题：机器学习的
一般原理 25
第2章 阴阳剖分：支持向量机模型 30
2.1 支持向量机模型的基本思路 30
2.1.1 支持向量机模型的
基本思路 31
2.1.2 支持向量机算法的
基本流程 34
2.2 数学形式与求解方法 34
2.2.1 数学知识补充 35
2.2.2 数学模型与理论推导 36
2.3 核方法与维度问题 38
2.3.1 核方法的含义 39
2.3.2 核函数SVM 39
2.4 软间隔支持向量机 41
2.4.1 软间隔的含义 41
2.4.2 软间隔SVM的损失
函数 42
2.5 本章小结与代码实现 44
2.6 本章话题：高维度，是灾难
还是契机？ 46
第3章 化直为曲：逻辑斯蒂回归 50
3.1 逻辑斯蒂回归的基本原理 50
3.1.1 分类问题与回归问题 51
3.1.2 逻辑斯蒂回归算法思路 53
3.2 逻辑斯蒂函数 56
3.2.1 逻辑斯蒂函数的由来 56
3.2.2 逻辑斯蒂函数的优势 58
3.3 逻辑斯蒂回归的数学原理 59
3.3.1 逻辑斯蒂回归的数学
形式 59
3.3.2 准确率和召回率 62
3.4 参数确定的方法 65
3.4.1 似然函数简介 65
3.4.2 逻辑斯蒂回归的损失
函数 66
3.5 多项逻辑斯蒂回归 67
3.5.1 多分类问题的逻辑斯蒂
回归 67
3.5.2 softmax函数 68
3.6 本章小结与代码实现 69
3.7 本章话题：广义线性模型 72
第4章 层层拷问：决策树模型 77
4.1 模型思路与算法流程 78
4.1.1 决策树的思路——
以读心术游戏为例 78
4.1.2 决策树模型的基本流程 81
4.1.3 决策树模型的关键问题 87
4.2 特征选择原则 87
4.2.1 信息增益原则 87
4.2.2 信息增益比原则 89
4.2.3 基尼系数原则 89
4.3 剪枝策略 90
4.4 常用决策树模型：ID3与
C4.5算法 92
4.4.1 ID3算法 92
4.4.2 C4.5算法 92
4.5 多变量决策树简介 93
4.6 本章小结与代码实现 94
4.7 本章话题：信息论与
特征选择 97
第5章 近朱者赤：k近邻模型 101
5.1 模型的思路和特点 101
5.1.1 模型思路 101
5.1.2 懒惰学习与迫切学习 103
5.2 模型的相关性质 105
5.2.1 数学形式 105
5.2.2 损失函数与误差 108
5.2.3 k近邻模型的改进 109
5.3 距离函数与参数选择 111
5.3.1 距离函数 111
5.3.2 参数选择的影响 114
5.4 本章小结与代码实现 115
5.5 本章话题：相似性度量 119
第6章 执果索因：朴素贝叶斯
模型 123
6.1 贝叶斯方法的基本概念 123
6.1.1 贝叶斯学派与频率
学派 124
6.1.2 全概率公式与贝叶斯
公式 127
6.2 朴素贝叶斯的原理和方法 133
6.2.1 朴素贝叶斯的“朴素”
假设 133
6.2.2 拉普拉斯平滑 135
6.3 朴素贝叶斯算法的步骤与
流程 137
6.4 生成式模型与判别式模型 138
6.5 本章小结与代码实现 138
6.6 本章话题：贝叶斯思维与
先验概念 141
第7章 提纲挈领：线性判别分析与
主成分分析 144
7.1 线性降维的基本思路 144
7.2 LDA 146
7.2.1 投影的技巧 146
7.2.2 类内距离和类间距离 147
7.2.3 LDA的求解 149
7.3 PCA 151
7.3.1 基变换与特征降维 151
7.3.2 方差最大化与PCA原理
推导 154
7.3.3 PCA的实现步骤 158
7.4 LDA与PCA：区别与
联系 158
7.5 本章小结与代码实现 159
7.5.1 LDA实验：鸢尾花
数据集降维分类 159
7.5.2 PCA实验：手写数字
数据集降维 161
7.6 本章话题：矩阵的直观解释
与应用 162
第8章 曲面平铺：流形学习 166
8.1 流形与流形学习 166
8.2 Isomap的基本思路与
实现方法 170
8.2.1 测地距离的概念 170
8.2.2 计算测地距离：图论中的
Floyd算法 172
8.2.3 由距离到坐标：多维尺度
变换方法 173
8.3 Isomap算法步骤 175
8.4 LLE的基本思路与
实现方法 175
8.4.1 LLE的基本思想 175
8.4.2 局部线性重构 176
8.5 LLE算法步骤 177
8.6 本章小结与代码实现 178
8.7 本章话题：黎曼、非欧几何
与流形感知 180
第9章 物以类聚：聚类算法 185
9.1 无监督方法概述 185
9.2 聚类的基本目标和评价
标准 187
9.2.1 聚类的基本目标 187
9.2.2 聚类的评价标准 188
9.3 基于中心的k-means
算法 191
9.3.1 k-means算法的基本
思路 191
9.3.2 k-means算法步骤 193
9.3.3 k-means算法的局
限性 195
9.4 层次聚类算法 196
9.4.1 层次聚类的基本原理 196
9.4.2 层次聚类的AGNES
算法 199
9.5 密度聚类算法：DBSCAN 200
9.5.1 DBSCAN算法的基本
思路 200
9.5.2 DBSCAN算法步骤 201
9.6 本章小结与代码实现 203
9.7 本章话题：Science上的一种
巧妙聚类算法 205
第10章 字典重构：稀疏编码 209
10.1 稀疏编码的思路 209
10.1.1 神经生物学的发现 210
10.1.2 过完备性与稀疏性 210
10.2 稀疏编码的数学形式 213
10.3 字典学习中的“字典” 215
10.3.1 传统算法中的
“字典” 215
10.3.2 “字典”学习的意义 216
10.4 本章小结与代码实现 217
10.5 本章话题：压缩感知理论
简介 220
第11章 教学相长：直推式支持
向量机 223
11.1 半监督学习简介 223
11.2 T-SVM模型 227
11.2.1 T-SVM的基本思路 227
11.2.2 T-SVM算法步骤 228
11.3 本章小结与代码实现 229
11.4 本章话题：不同样本集场景
下的问题处理策略 233
第12章 群策群力：集成学习 236
12.1 自举汇聚和提升 236
12.1.1 Bagging算法和Boosting
算法的基本思路 237
12.1.2 Bagging算法和Boosting
算法的区别与联系 240
12.2 Bagging算法的基本
步骤 241
12.3 Boosting算法的基本
步骤 242
12.4 Bagging算法：以随机
森林算法为例 243
12.4.1 随机森林算法 243
12.4.2 随机森林算法中的
随机性 244
12.5 Boosting算法：以Adaboost
算法为例 244
12.5.1 Adaboost算法的实现
步骤 245
12.5.2 Adaboost算法过程
分析 245
12.6 本章小结与代码实现 246
12.7 本章话题：Adaboost算法
中的分步策略 249
第二篇 深度学习模型与方法 
第13章 神经网络与深度学习：从感知机模型到阿尔法狗 254
13.1 感知机模型 256
13.1.1 感知机模型的基本原理
与数学形式 256
13.1.2 感知机模型的缺陷与
改进 260
13.2 人工神经网络 262
13.2.1 生物神经元与感知机
模型 262
13.2.2 人工神经网络方法
简介 264
13.2.3 反向传播算法 265
13.2.4 神经网络的优势 267
13.3 需要深度学习的原因 268
13.4 神经网络模型的局限性 268
13.5 常用神经网络框架简介 270
13.6 本章话题：人工智能发展
大事年表 271
 显示全部信息